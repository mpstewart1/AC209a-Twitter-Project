{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1023
    },
    "colab_type": "code",
    "id": "9BQ1yvupbroN",
    "outputId": "724fdb89-930d-41b3-e193-d5c8f7e142d8"
   },
   "outputs": [],
   "source": [
    "# !pip install tweepy\n",
    "# !pip install nltk\n",
    "# !pip install twython\n",
    "# !pip install jsonpickle\n",
    "# !pip install botometer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qw42mGbHbroT"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "import tweepy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import botometer\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NcBUWLfJbroU"
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('pr0AH7Ot5sZmig4u3bA6j51ty', 'tNteF0tRlEjKJfkkWQaIv5myqT9oBqrIVOYPQJOMjBTJhn9SAF')\n",
    "auth.set_access_token('934846563825930241-yO5rosUB4x8eFMO0J7IXV1UZM0RzbgL', 'CbqfvlRonXo2JiIyxqCqeZynwkslNcDPmGFQ9KBEh8Mch')\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "mashape_key = \"uIX3UUkrh7mshux9VLXhN1FcUYY0p1ZEJpCjsnCHKddXFfIzhf\"\n",
    "twitter_app_auth = {\n",
    "    'consumer_key': 'pr0AH7Ot5sZmig4u3bA6j51ty',\n",
    "    'consumer_secret': 'tNteF0tRlEjKJfkkWQaIv5myqT9oBqrIVOYPQJOMjBTJhn9SAF',\n",
    "    'access_token': '934846563825930241-yO5rosUB4x8eFMO0J7IXV1UZM0RzbgL',\n",
    "    'access_token_secret': 'CbqfvlRonXo2JiIyxqCqeZynwkslNcDPmGFQ9KBEh8Mch',\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyoKqy1ebroX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Botometer\n",
    "\n",
    "bom = botometer.Botometer(wait_on_ratelimit=True,mashape_key=mashape_key,**twitter_app_auth)\n",
    "# Check a single account by screen name\n",
    "result1 = bom.check_account('@clayadavis')\n",
    "\n",
    "# Check a single account by id\n",
    "result2 = bom.check_account(1548959833)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9VUWF328broc"
   },
   "outputs": [],
   "source": [
    "# Social Honeypot Dataset\n",
    "\n",
    "# Legitimate user info\n",
    "lu_df = pd.read_csv('legitimate_users.txt', sep = '\\t', header = None)\n",
    "lu_df.columns = ['UserID', 'CreatedAt', 'CollectedAt', 'NumerOfFollowings', 'NumberOfFollowers', 'NumberOfTweets', 'LengthOfScreenName', 'LengthOfDescriptionInUserProfile']\n",
    "# Content polluters info\n",
    "bots_df = pd.read_csv('bot_users.txt', sep = '\\t', header = None)\n",
    "bots_df.columns = ['UserID', 'CreatedAt', 'CollectedAt', 'NumerOfFollowings', 'NumberOfFollowers', 'NumberOfTweets', 'LengthOfScreenName', 'LengthOfDescriptionInUserProfile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4lanZnXbroe",
    "outputId": "503a642f-0d3c-4a0a-9012-a283b7e0ecd3"
   },
   "outputs": [],
   "source": [
    "lu_list = lu_df['UserID'].values.astype(int)\n",
    "bot_list = bots_df['UserID'].values.astype(int)\n",
    "lu_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "colab_type": "code",
    "id": "xoHzKUvBbroi",
    "outputId": "316321ca-5004-44d8-d2ea-2e2dcb792d00"
   },
   "outputs": [],
   "source": [
    "# Given a name list and number of tweets needed to extract for each account\n",
    "# Return a dictionary of dataframes\n",
    "# Each dataframe contains info of one user\n",
    "def API_scrap(name_list, count_num):\n",
    "    fail_lst = []\n",
    "    user_dfs = {}\n",
    "    for name in name_list:\n",
    "        print(name)\n",
    "        try:\n",
    "            status_a = api.user_timeline(name, count = count_num, tweet_mode = 'extended')\n",
    "            user_dfs[name] = pd.DataFrame()\n",
    "            for i in range(len(status_a)):\n",
    "                json_str = json.dumps(status_a[i]._json)\n",
    "                jdata = json_normalize(json.loads(json_str))\n",
    "                user_dfs[name] = user_dfs[name].append(jdata, ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            fail_lst.append(name)\n",
    "            continue\n",
    "    \n",
    "    return user_dfs, fail_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dfs, fail_lst = API_scrap(lu_list[0:100], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "successful_users = [name for name in user_dfs.keys() if name not in fail_lst]\n",
    "successful_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_dfs[successful_users[0]].columns[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "single_user = user_dfs[successful_users[0]][['full_text','created_at',\n",
    "                                             'entities.user_mentions']].copy().reset_index()\n",
    "single_user['user_id'] = successful_users[0]\n",
    "single_user.drop(['index'], inplace=True, axis = 1)\n",
    "\n",
    "display(single_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(successful_users):\n",
    "    try:\n",
    "        single_user = user_dfs[successful_users[i]][['full_text','created_at','entities.user_mentions']].copy().reset_index()\n",
    "        single_user['user_id'] = successful_users[i]\n",
    "        single_user.drop(['index'], inplace=True, axis = 1)\n",
    "        filepath ='data_NLP/bots/{}_tweets.csv'.format(name)\n",
    "        single_user.to_csv(filepath, sep='\\t')\n",
    "    except:\n",
    "        print(\"couldnt do user {} for some reason\".format(successful_users[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rc_RM_jVbron"
   },
   "outputs": [],
   "source": [
    "# get some bots! \n",
    "bot_dfs, bot_fail_lst = API_scrap(bot_list[0:100], 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7j58UA1Ybrop",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "successful_users_bots = [name for name in bot_dfs.keys() if name not in bot_fail_lst]\n",
    "successful_users_bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldnt do user 8377772 for some reason\n",
      "couldnt do user 7721172 for some reason\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(successful_users_bots):\n",
    "    try:\n",
    "        single_user = bot_dfs[successful_users_bots[i]][['full_text','created_at','entities.user_mentions']].copy().reset_index()\n",
    "        single_user['user_id'] = successful_users_bots[i]\n",
    "        single_user.drop(['index'], inplace=True, axis = 1)\n",
    "        filepath ='data_NLP/bots/{}_tweets.csv'.format(name)\n",
    "        single_user.to_csv(filepath, sep='\\t')\n",
    "    except:\n",
    "        print(\"couldnt do user {} for some reason\".format(successful_users_bots[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AM207_HW9_2018_Matthew-Stewart.ipynb \u001b[31mbot_users.txt\u001b[m\u001b[m\r\n",
      "BarackObama_tweets.csv               \u001b[34mcresci-2017.csv (1)\u001b[m\u001b[m\r\n",
      "LICENSE                              \u001b[34mdata_NLP\u001b[m\u001b[m\r\n",
      "NLP_EDA.ipynb                        elonmusk_tweets.csv\r\n",
      "README.md                            \u001b[31mlegitimate_users.txt\u001b[m\u001b[m\r\n",
      "Twitter_data_datascrape.ipynb        tweets.json\r\n",
      "_config.yml\r\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "!ls\n",
    "mypath_bots = 'data_NLP/bots/'\n",
    "mypath_legit = 'data_NLP/legit/'\n",
    "botfiles = [f for f in listdir(mypath_bots) if isfile(join(mypath_bots, f)) and not f=='.DS_Store']\n",
    "legitfiles = [f for f in listdir(mypath_bots) if isfile(join(mypath_bots, f))and not f=='.DS_Store']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code on one dataframe \n",
    "tweets_df = pd.DataFrame.from_csv('data_NLP/bots/' + botfiles[0],sep='\\t')\n",
    "tweets_df.rename(index=str, columns={\"full_text\": \"text\"}, inplace=True)\n",
    "\n",
    "tweets_df['created_at'] = pd.to_datetime(tweets_df['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NLP_dataframe(list_of_users, botBool):\n",
    "    \n",
    "    tweets_df = pd.DataFrame.from_csv('data_NLP/bots/' + list_of_users[0] ,sep='\\t')\n",
    "    tweets_df.rename(index=str, columns={\"full_text\": \"text\"}, inplace=True)\n",
    "\n",
    "    tweets_df['created_at'] = pd.to_datetime(tweets_df['created_at'])\n",
    "    \n",
    "    if botBool:\n",
    "        tweets_df['botBool'] = 1\n",
    "    else:\n",
    "        tweets_df['botBool'] = 0\n",
    "\n",
    "    # number of hashtags \n",
    "    tweets_df['num_hashtags'] = tweets_df['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n",
    "\n",
    "    # number of all-caps words \n",
    "    tweets_df['num_upper'] = tweets_df['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
    "\n",
    "    # deal with emojis\n",
    "    import emoji\n",
    "\n",
    "    class Emoticons:\n",
    "        POSITIVE = [\"*O\", \"*-*\", \"*O*\", \"*o*\", \"* *\",\n",
    "                    \":P\", \":D\", \":d\", \":p\",\n",
    "                    \";P\", \";D\", \";d\", \";p\",\n",
    "                    \":-)\", \";-)\", \":=)\", \";=)\",\n",
    "                    \":<)\", \":>)\", \";>)\", \";=)\",\n",
    "                    \"=}\", \":)\", \"(:;)\",\n",
    "                    \"(;\", \":}\", \"{:\", \";}\",\n",
    "                    \"{;:]\",\n",
    "                    \"[;\", \":')\", \";')\", \":-3\",\n",
    "                    \"{;\", \":]\",\n",
    "                    \";-3\", \":-x\", \";-x\", \":-X\",\n",
    "                    \";-X\", \":-}\", \";-=}\", \":-]\",\n",
    "                    \";-]\", \":-.)\",\n",
    "                    \"^_^\", \"^-^\"]\n",
    "\n",
    "        NEGATIVE = [\":(\", \";(\", \":'(\",\n",
    "                    \"=(\", \"={\", \"):\", \");\",\n",
    "                    \")':\", \")';\", \")=\", \"}=\",\n",
    "                    \";-{{\", \";-{\", \":-{{\", \":-{\",\n",
    "                    \":-(\", \";-(\",\n",
    "                    \":,)\", \":'{\",\n",
    "                    \"[:\", \";]\"\n",
    "                    ]\n",
    "\n",
    "    def getPositiveTweetEmojis(tweet):\n",
    "        return ''.join(c for c in tweet if c in Emoticons.POSITIVE)\n",
    "\n",
    "    def getNegativeTweetEmojis(tweet):\n",
    "        return ''.join(c for c in tweet if c in Emoticons.NEGATIVE)\n",
    "\n",
    "    def extractAllEmojis(str):\n",
    "        return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "\n",
    "    # all emojis in a text \n",
    "    def extract_emojis(str):\n",
    "        return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "    tweets_df['all_emojis'] = [extractAllEmojis(tweet) for tweet in tweets_df['text']]\n",
    "    tweets_df['positive_emojis'] = [getPositiveTweetEmojis(tweet) for tweet in tweets_df['text']]\n",
    "    tweets_df['negative_emojis'] = [getNegativeTweetEmojis(tweet) for tweet in tweets_df['text']]\n",
    "\n",
    "    # clean tweets \n",
    "    from bs4 import BeautifulSoup\n",
    "    tweets_df['text'] = [re.sub(r'http[A-Za-z0-9:/.]+','',str(tweets_df['text'][i])) for i in range(len(tweets_df['text']))]\n",
    "    removeHTML_text = [BeautifulSoup(tweets_df.text[i], 'lxml').get_text() for i in range(len(tweets_df.text))]\n",
    "    tweets_df.text = removeHTML_text\n",
    "    tweets_df['text'] = [re.sub(r'@[A-Za-z0-9]+','',str(tweets_df['text'][i])) for i in range(len(tweets_df['text']))]\n",
    "\n",
    "    weird_characters_regex = re.compile(r\"[^\\w\\d ]\")\n",
    "    tweets_df.text = tweets_df.text.str.replace(weird_characters_regex, \"\")\n",
    "    RT_bool = [1 if text[0:2]=='RT' else 0 for text in tweets_df['text']]\n",
    "    tweets_df['RT'] = RT_bool\n",
    "    tweets_df.text = tweets_df.text.str.replace('RT', \"\")\n",
    "\n",
    "    # average time between retweets\n",
    "    retweet_table = tweets_df[['created_at','RT']].copy()\n",
    "    retweet_table  = retweet_table[retweet_table.RT == 1]\n",
    "    total_observation_period_rt = retweet_table['created_at'][0]-retweet_table['created_at'][-1]\n",
    "    time_between_average_rt = float(len(retweet_table))/total_observation_period_rt.days\n",
    "\n",
    "    # average number of mentions\n",
    "    tweets_df['num_mentions'] = [len(eval(tweets_df['entities.user_mentions'][i])) for i in range(len(tweets_df.text))]\n",
    "\n",
    "    # average time between mentions\n",
    "    mention_table = tweets_df[['num_mentions','created_at']].copy()\n",
    "    mention_table  = mention_table[mention_table['num_mentions']>0]\n",
    "    mention_table.head()\n",
    "    total_observation_period_mention = mention_table['created_at'][0]-mention_table['created_at'][-1]\n",
    "    time_between_average_mention = float(len(mention_table))/total_observation_period_mention.days\n",
    "    #print(time_between_average_rt,time_between_average_mention)\n",
    "    \n",
    "    # get word count, char count\n",
    "    tweets_df['word_count'] = tweets_df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "    tweets_df['char_count'] = tweets_df['text'].str.len() ## this also includes spaces\n",
    "    #display(tweets_df[['text','char_count','word_count']].head())\n",
    "\n",
    "\n",
    "    # build some average features \n",
    "    tweets_df['avg_time_between_rt'] = time_between_average_rt\n",
    "    tweets_df['avg_time_between_mention'] = time_between_average_mention\n",
    "    tweets_df['avg_num_mentions'] = np.mean(tweets_df['num_mentions'])\n",
    "    tweets_df['avg_num_hashtags'] = np.mean(tweets_df['num_hashtags'])\n",
    "    tweets_df['avg_num_caps'] = np.mean(tweets_df['num_upper'])\n",
    "    tweets_df['avg_words_per_tweet'] = np.mean(tweets_df['word_count'])\n",
    "    tweets_df['emoji_bool'] = [1 if len(tweets_df['all_emojis'][i])>0 else 0 for i in range(len(tweets_df))]\n",
    "    tweets_df['emoji_p_bool'] = [1 if len(tweets_df['positive_emojis'][i])>0 else 0 for i in range(len(tweets_df))]\n",
    "    tweets_df['emoji_n_bool'] = [1 if len(tweets_df['negative_emojis'][i])>0 else 0 for i in range(len(tweets_df))]\n",
    "    tweets_df['emoji_pn_bool'] = [1 if len(tweets_df['positive_emojis'][i])>0 and \n",
    "                                  len(tweets_df['negative_emojis'][i])>0 else 0 for i in range(len(tweets_df))]\n",
    "    tweets_df['percent_with_emoji'] = np.mean(tweets_df['emoji_bool'])\n",
    "    tweets_df['percent_with_p_emoji'] = np.mean(tweets_df['emoji_p_bool'])\n",
    "    tweets_df['percent_with_n_emoji'] = np.mean(tweets_df['emoji_n_bool'])\n",
    "    tweets_df['percent_with_pn_emoji'] = np.mean(tweets_df['emoji_pn_bool'])\n",
    "\n",
    "\n",
    "    # check data types \n",
    "    display(tweets_df.dtypes)\n",
    "\n",
    "    # get average word length\n",
    "    def avg_word(sentence):\n",
    "        words = sentence.split()\n",
    "        if len(words) == 0:\n",
    "            return 0\n",
    "        return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "    tweets_df['avg_word'] = tweets_df['text'].apply(lambda x: avg_word(x))\n",
    "    tweets_df[['text','avg_word']].head()\n",
    "\n",
    "    # all lowercase \n",
    "    tweets_df['text'] = tweets_df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    display(tweets_df['text'].head())\n",
    "\n",
    "    # remove stopwords and punctuation\n",
    "    from nltk.corpus import stopwords\n",
    "    from string import punctuation\n",
    "\n",
    "    retweet = ['RT','rt']\n",
    "    stoplist = stopwords.words('english') + list(punctuation) + retweet\n",
    "    #stop = stopwords.words('english')\n",
    "    tweets_df['text'] = tweets_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stoplist))\n",
    "    tweets_df['text'].head()\n",
    "\n",
    "    # add sentiment feature\n",
    "    from textblob import Word, TextBlob  \n",
    "    tweets_df['sentiment'] = tweets_df['text'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "    tweets_df['polarity'] = tweets_df['text'].apply(lambda x: TextBlob(x).sentiment[1])\n",
    "    tweets_df = tweets_df.sort_values(['sentiment'])\n",
    "\n",
    "    # add list of nouns\n",
    "    import nltk\n",
    "\n",
    "    tweets_df['nouns'] = [TextBlob(tweets_df.text[i]).noun_phrases for i in range(len(tweets_df.text))]\n",
    "\n",
    "    tweets_df['POS_tag_list'] = [TextBlob(tweets_df.text[i]).tags for i in range(len(tweets_df.text))]\n",
    "    tweets_df['POS_tag_list'] = [[tuple_POS[1] for tuple_POS in tweets_df['POS_tag_list'][i]] for i in range(len(tweets_df.text))]\n",
    "\n",
    "    # look at 10 most frequent words\n",
    "    freq = pd.Series(' '.join(tweets_df['text']).split()).value_counts()[:20]\n",
    "\n",
    "    tweets_df['top_20_nouns'] = [freq.index.values for i in range(len(tweets_df))]\n",
    "\n",
    "    list_of_POS = [\"CC\",\"CD\",\"DT\",\"EX\",\"FW\",\"IN\",\"JJ\",\"JJR\",\"JJS\",\"LS\",\"MD\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\n",
    "    \"PDT\",\"POS\",\"PRP\",\"PRP\",\"RB\",\"RBR\",\"RBS\",\"RP\",\"TO\",\"UH\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\n",
    "    \"VBP\",\"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\"]\n",
    "\n",
    "    for POS in list_of_POS:\n",
    "        varname = POS+\"_count\"\n",
    "        tweets_df[varname] = [tweets_df['POS_tag_list'][i].count(POS) for i in range(len(tweets_df.text))]\n",
    "        \n",
    "    # get full text in a string\n",
    "    full_tweet_text = \"\"\n",
    "    for i in range(len(tweets_df)):\n",
    "        full_tweet_text = full_tweet_text + tweets_df['text'][i]\n",
    "    full_tweet_text_list = full_tweet_text.split()\n",
    "    unique_full_text = len(set(full_tweet_text_list))\n",
    "        \n",
    "    # features based on full text \n",
    "    tweets_df['word_diversity'] = unique_full_text/len(full_tweet_text)\n",
    "    tweets_df['overall_sentiment'] = TextBlob(full_tweet_text).sentiment[0]\n",
    "    tweets_df['overall_polarity'] = TextBlob(full_tweet_text).sentiment[1]\n",
    "        \n",
    "    subset_df= tweets_df[['word_diversity','overall_sentiment','overall_polarity']]\n",
    "    return subset_df.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data_NLP/bots/7' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-374-af3544a06216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_NLP_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbotfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbotfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-374-af3544a06216>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbot_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcreate_NLP_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbotfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbotfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-369-079eab89d369>\u001b[0m in \u001b[0;36mcreate_NLP_dataframe\u001b[0;34m(list_of_users, botBool)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_NLP_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbotBool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data_NLP/bots/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist_of_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"full_text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[0;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[1;32m   1577\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'block'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data_NLP/bots/7' does not exist"
     ]
    }
   ],
   "source": [
    "bot_data = [create_NLP_dataframe(botfiles[i], 1) for i in range(len(botfiles))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities.user_mentions</th>\n",
       "      <th>user_id</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>num_upper</th>\n",
       "      <th>all_emojis</th>\n",
       "      <th>positive_emojis</th>\n",
       "      <th>negative_emojis</th>\n",
       "      <th>RT</th>\n",
       "      <th>...</th>\n",
       "      <th>VB_count</th>\n",
       "      <th>VBD_count</th>\n",
       "      <th>VBG_count</th>\n",
       "      <th>VBN_count</th>\n",
       "      <th>VBP_count</th>\n",
       "      <th>VBZ_count</th>\n",
       "      <th>WDT_count</th>\n",
       "      <th>WP_count</th>\n",
       "      <th>WP$_count</th>\n",
       "      <th>WRB_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>thing bad technology writes managing director</td>\n",
       "      <td>2018-11-08 16:39:01</td>\n",
       "      <td>[{'screen_name': 'Campaignmag', 'id_str': '164...</td>\n",
       "      <td>7087112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>cma probe influencers fail declare payment end...</td>\n",
       "      <td>2018-08-17 13:35:02</td>\n",
       "      <td>[]</td>\n",
       "      <td>7087112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>cma probe influencers fail declare payment end...</td>\n",
       "      <td>2018-08-25 14:00:23</td>\n",
       "      <td>[]</td>\n",
       "      <td>7087112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>cma probe influencers fail declare payment end...</td>\n",
       "      <td>2018-08-24 01:44:02</td>\n",
       "      <td>[]</td>\n",
       "      <td>7087112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>cma probe influencers fail declare payment end...</td>\n",
       "      <td>2018-08-19 16:02:07</td>\n",
       "      <td>[]</td>\n",
       "      <td>7087112</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text          created_at  \\\n",
       "14       thing bad technology writes managing director 2018-11-08 16:39:01   \n",
       "174  cma probe influencers fail declare payment end... 2018-08-17 13:35:02   \n",
       "98   cma probe influencers fail declare payment end... 2018-08-25 14:00:23   \n",
       "118  cma probe influencers fail declare payment end... 2018-08-24 01:44:02   \n",
       "156  cma probe influencers fail declare payment end... 2018-08-19 16:02:07   \n",
       "\n",
       "                                entities.user_mentions  user_id  num_hashtags  \\\n",
       "14   [{'screen_name': 'Campaignmag', 'id_str': '164...  7087112             0   \n",
       "174                                                 []  7087112             0   \n",
       "98                                                  []  7087112             0   \n",
       "118                                                 []  7087112             0   \n",
       "156                                                 []  7087112             0   \n",
       "\n",
       "     num_upper all_emojis positive_emojis negative_emojis  RT    ...      \\\n",
       "14           1                                              1    ...       \n",
       "174          1                                              0    ...       \n",
       "98           1                                              0    ...       \n",
       "118          1                                              0    ...       \n",
       "156          1                                              0    ...       \n",
       "\n",
       "     VB_count  VBD_count  VBG_count  VBN_count  VBP_count  VBZ_count  \\\n",
       "14          0          0          1          0          0          1   \n",
       "174         0          0          0          0          1          0   \n",
       "98          0          0          0          0          1          0   \n",
       "118         0          0          0          0          1          0   \n",
       "156         0          0          0          0          1          0   \n",
       "\n",
       "    WDT_count WP_count  WP$_count  WRB_count  \n",
       "14          0        0          0          0  \n",
       "174         0        0          0          0  \n",
       "98          0        0          0          0  \n",
       "118         0        0          0          0  \n",
       "156         0        0          0          0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust spelling using TextBlob\n",
    "# run once when data are ready- takes a long time! \n",
    "#tweets_df['text'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['lining', '84']),\n",
       " WordList(['84', 'mannequin']),\n",
       " WordList(['mannequin', 'top']),\n",
       " WordList(['top', 'building']),\n",
       " WordList(['building', 'creating']),\n",
       " WordList(['creating', 'vineyard']),\n",
       " WordList(['vineyard', 'middle']),\n",
       " WordList(['middle', 'railway']),\n",
       " WordList(['railway', 'station']),\n",
       " WordList(['station', 'making']),\n",
       " WordList(['making', 'burger']),\n",
       " WordList(['burger', 'billboard']),\n",
       " WordList(['billboard', 'find']),\n",
       " WordList(['find', 'branded']),\n",
       " WordList(['branded', 'experience']),\n",
       " WordList(['experience', 'continue']),\n",
       " WordList(['continue', 'provoke']),\n",
       " WordList(['provoke', 'inspire']),\n",
       " WordList(['inspire', 'engage']),\n",
       " WordList(['engage', '2018'])]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization\n",
    "TextBlob(tweets_df['text'][0]).words\n",
    "\n",
    "# convert word to base form\n",
    "tweets_df['text'] = tweets_df['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "# build n-grams. That is, build some structures that often come together. For example, for Elon Musk, \"Tesla 3.\"\n",
    "# or for Trump, \"build the wall,\" etc. \n",
    "\n",
    "TextBlob(tweets_df['text'][0]).ngrams(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cma</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>declare</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>endorsement</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fail</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>influencers</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>payment</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>probe</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>adblocking</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>behind</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>fall</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words   tf\n",
       "11          cma  5.0\n",
       "8       declare  5.0\n",
       "12  endorsement  5.0\n",
       "9          fail  5.0\n",
       "6   influencers  5.0\n",
       "10      payment  5.0\n",
       "7         probe  5.0\n",
       "30   adblocking  2.0\n",
       "32       behind  2.0\n",
       "34         fall  2.0"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF explained https://www.analyticsvidhya.com/blog/2015/04/information-retrieval-system-explained/\n",
    "\n",
    "# sample term frequency table\n",
    "tf1 = (tweets_df['text'][0:10]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
    "tf1.columns = ['words','tf']\n",
    "tf1.sort_values(['tf', 'words'], ascending=[0, 1]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tweets_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate which words appear frequently across texts -- these words don't give us much information (we,great,how,etc.)\n",
    "for i,word in enumerate(tf1['words']):\n",
    "    tf1.loc[i, 'idf'] = np.log(tweets_df.shape[0]/(len(tweets_df[tweets_df['text'].str.contains(word)])))\n",
    "display(tf1.sort_values(['idf'],ascending=[1]).head(10))\n",
    "\n",
    "tf1['tfidf'] = tf1['tf'] * tf1['idf']\n",
    "display(tf1.sort_values(['tfidf'],ascending=[1]).head(10))\n",
    "\n",
    "# can also do this with sklearn, see code below\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# tfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n",
    "#  stop_words= 'english',ngram_range=(1,1))\n",
    "# train_vect = tfidf.fit_transform(tweets_df['text'])\n",
    "\n",
    "# train_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build corpus\n",
    "import tempfile\n",
    "import logging\n",
    "from gensim import corpora\n",
    "\n",
    "corpus=[]\n",
    "a=[]\n",
    "for i in range(len(tweets_df['text'])):\n",
    "        a=tweets_df['text'][i]\n",
    "        corpus.append(a)\n",
    "        \n",
    "# look at first 5 lines        \n",
    "corpus[0:5]\n",
    "\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# remove common words and tokenize\n",
    "# uncomment and download stopwords if nessisary\n",
    "# nltk.download()\n",
    "list1 = ['RT','rt']\n",
    "stoplist = stopwords.words('english') + list(punctuation) + list1\n",
    "\n",
    "# tokenize words \n",
    "texts = [[word for word in str(document).split()] for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dictionary of words, save it \n",
    "from collections import OrderedDict\n",
    "from gensim import corpora, models, similarities\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'twitter.dict'))\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'twitter.mm'), corpus)  # store to disk, for lda\n",
    "\n",
    "# TfidfModel: multiplies a local component (term frequency) with a global component \n",
    "# (inverse document frequency), and normalizing the resulting documents to unit length\n",
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]  # step 2 -- use the model to transform vectors\n",
    "\n",
    "total_topics = 5\n",
    "lda = models.LdaModel(corpus, id2word=dictionary, num_topics=total_topics)\n",
    "corpus_lda = lda[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "\n",
    "#Show first n important word in the topics:\n",
    "lda.show_topics(total_topics,5)\n",
    "\n",
    "data_lda = {i: OrderedDict(lda.show_topic(i,25)) for i in range(total_topics)}\n",
    "\n",
    "#made dataframe\n",
    "df_lda = pd.DataFrame(data_lda)\n",
    "print(df_lda.shape)\n",
    "df_lda = df_lda.fillna(0).T\n",
    "print(df_lda.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "g=sns.clustermap(df_lda.corr(), center=0, cmap=\"RdBu\", metric='cosine', linewidths=.75, figsize=(12, 12))\n",
    "plt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at results\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(lda, corpus_lda, dictionary, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter_data.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
