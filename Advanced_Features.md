---
nav_include: 5
title: Advanced Topics
notebook: Advanced_Features.ipynb
---

## Contents
{:.no_toc}
*  
{: toc}

## Overview

Several advanced models were implemented for this project, including:

- Stacking (Meta Ensembling)
- Blended Ensemble
- Regularized Neural Network
- K Means

## Important Packages

In addition to the packages described in the model development package, additional models were used for implementing the advanced models.

1. `Keras` - **Neural network library for Python** - [Documentation](https://keras.io/)

2. `Tensorflow` - **Open source machine learning framework** - [Documentation](https://www.tensorflow.org/)

3. `mlens` - **Python library for memory efficient parallelized ensemble learning** - [Documentation](https://mlens.readthedocs.io/en/0.1.x/)


## Stacked Model

Model stacking is an efficient ensemble method in which the predictions, generated by using various machine learning algorithms, are used as inputs in a second-layer learning algorithm. This second-layer algorithm is trained to optimally combine the model predictions to form a new set of predictions. For example, when linear regression is used as second-layer modeling, it estimates these weights by minimizing the least square errors. However, the second-layer modeling is not restricted to only linear models; the relationship between the predictors can be more complex, opening the door to employing other machine learning algorithms.

Ensemble modeling and model stacking are especially popular in data science competitions, in which a sponsor posts a training set (which includes labels) and a test set (which does not include labels) and issues a global challenge to produce the best predictions of the test set for a specified performance criterion. The winning teams almost always use ensemble models instead of a single fine-tuned model. Often individual teams develop their own ensemble models in the early stages of the competition, and then join their forces in the later stages. 

A simple way to enhance diversity is to train models by using different machine learning algorithms. For example, adding a factorization model to a set of tree-based models (such as random forest and gradient boosting) provides a nice diversity because a factorization model is trained very differently than decision tree models are trained. For the same machine learning algorithm, you can enhance diversity by using different hyperparameter settings and subsets of variables. If you have many features, one efficient method is to choose subsets of the variables by simple random sampling.

Overfitting is an especially big problem in model stacking, because so many predictors that all predict the same target are combined. Overfitting is partially caused by this collinearity between the predictors. The most efficient techniques for training models (especially during the stacking stages) include using cross validation and some form of regularization. A good paper that outlines this procedure is [Stacked Ensemble Models for Improved Prediction Accuracy](https://support.sas.com/resources/papers/proceedings17/SAS0437-2017.pdf).

That paper also shows how you can generate a diverse set of models by various methods (such as forests, gradient boosted decision trees, factorization machines, and logistic regression) and then combine them with stacked ensemble techniques such regularized regression methods, gradient boosting, and hill climbing methods.

Applying stacked models to real-world big data problems can produce greater prediction accuracy and robustness than do individual models. The model stacking approach is powerful and compelling enough to alter your initial data mining mindset from finding the single best model to finding a collection of really good complementary models. Of course, this method does involve additional cost both because you need to train a large number of models and because you need to use cross validation to avoid overfitting.

In this section we will try to implement a stacked model similar to that proposed in the "[Stacked Ensemble Models for Improved Prediction Accuracy](https://support.sas.com/resources/papers/proceedings17/SAS0437-2017.pdf)" paper.

```python
# Going to use these 5 base models for the stacking
from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, 
                              GradientBoostingClassifier, ExtraTreesClassifier)
from sklearn.svm import SVC
```

### Helpers via Python Classes

In the section of code below, we essentially write a class SklearnHelper that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn classifiers. Therefore this cuts out redundancy as won't need to write the same methods five times if we wanted to invoke five different classifiers.

```python
# Some useful parameters which will come in handy later on
ntrain = X_train_scaled.shape[0]
ntest = X_test_scaled.shape[0]
SEED = 99 # for reproducibility
NFOLDS = 5 # set folds for out-of-fold prediction

kf = KFold(n_splits = NFOLDS, random_state=SEED)

# Class to extend the Sklearn classifier
class SklearnHelper(object):
    def __init__(self, clf, seed=0, params=None):
        params['random_state'] = seed
        self.clf = clf(**params)

    def train(self, x_train, y_train):
        self.clf.fit(x_train, y_train)

    def predict(self, x):
        return self.clf.predict(x)
    
    def fit(self,x,y):
        return self.clf.fit(x,y)
    
    def feature_importances(self,x,y):
            print(self.clf.fit(x,y).feature_importances_)
    
# Class to extend XGboost classifer
```

**def init :** Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).

The rest of the code are simply methods of the class which simply call the corresponding methods already existing within the sklearn classifiers. Essentially, we have created a wrapper class to extend the various Sklearn classifiers so that this should help us reduce having to write the same code over and over when we implement multiple learners to our stacker.

### Out-of-Fold Predictions

Stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having "seen" the test set and therefore overfitting when feeding these predictions.



```python
def get_oof(clf, x_train, y_train, x_test):
    oof_train = np.zeros((ntrain,))
    oof_test = np.zeros((ntest,))
    oof_test_skf = np.empty((NFOLDS, ntest))

    for i, (train_index, test_index) in enumerate(kf.split(x_train)):

        x_tr = x_train.iloc[train_index]
        y_tr = y_train.iloc[train_index]
        x_te = x_train.iloc[test_index]
        clf.train(x_tr, y_tr)

        oof_train[test_index] = clf.predict(x_te)
        oof_test_skf[i, :] = clf.predict(x_test)

    oof_test[:] = oof_test_skf.mean(axis=0)
    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)
```


### Generating our Base First-Level Models

So now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:

- **Random Forest classifier**
- **Extra Trees classifier**
- **AdaBoost classifer**
- **Gradient Boosting classifer**
- **Support Vector Machine**

**Parameters**

**n_jobs :** Number of cores used for the training process. If set to -1, all cores are used.

**n_estimators :** Number of classification trees in your learning model ( set to 10 per default)

**max_depth :** Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep

**verbose :** Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.



```python
# Put in our parameters for said classifiers
# Random Forest parameters
rf_params = {
    'n_jobs': -1,
    'n_estimators': 500,
     'warm_start': True, 
     #'max_features': 0.2,
    'max_depth': 6,
    'min_samples_leaf': 2,
    'max_features' : 'sqrt',
    'verbose': 0
}

# Extra Trees Parameters
et_params = {
    'n_jobs': -1,
    'n_estimators':500,
    #'max_features': 0.5,
    'max_depth': 4,
    'min_samples_leaf': 2,
    'verbose': 0
}

# AdaBoost parameters
ada_params = {
    'n_estimators': 500,
    'learning_rate' : 0.75
}

# Gradient Boosting parameters
gb_params = {
    'n_estimators': 500,
     #'max_features': 0.2,
    'max_depth': 5,
    'min_samples_leaf': 2,
    'verbose': 0
}

# Support Vector Classifier parameters 
svc_params = {
    'kernel' : 'sigmoid',
    'C' : 0.1
    }

logreg_params = {'C':100000, 'fit_intercept': True}
lda_params = {'store_covariance': True}
qda_params = {'store_covariance': True}
polylogreg_params = {}
```


Let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.



```python
# Create 5 objects that represent our 4 models
rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)
et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)
ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)
gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)
svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)
logreg_stack = SklearnHelper(clf=LogisticRegression, seed=SEED, params=logreg_params)
```

### Output of the First level Predictions

We now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run.



```python
# Create our OOF train and test predictions. These base results will be used as new features
et_oof_train, et_oof_test = get_oof(et, X_train_scaled, Y_train, X_test_scaled) # Extra Trees
rf_oof_train, rf_oof_test = get_oof(rf,X_train_scaled, Y_train, X_test_scaled) # Random Forest
ada_oof_train, ada_oof_test = get_oof(ada, X_train_scaled, Y_train, X_test_scaled) # AdaBoost 
gb_oof_train, gb_oof_test = get_oof(gb,X_train_scaled, Y_train, X_test_scaled) # Gradient Boost
svc_oof_train, svc_oof_test = get_oof(svc,X_train_scaled, Y_train, X_test_scaled) # Support Vector Classifier
logreg_oof_train, logreg_oof_test = get_oof(logreg_stack,X_train_scaled, Y_train, X_test_scaled) # Logistic regression
```
    

### Feature importances generated from the different classifiers

Now having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.

As per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in .featureimportances. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such

```python
rf_feature = rf.feature_importances(X_train_scaled,Y_train);
et_feature = et.feature_importances(X_train_scaled, Y_train);
ada_feature = ada.feature_importances(X_train_scaled, Y_train);
gb_feature = gb.feature_importances(X_train_scaled,Y_train);
```


    [0.01040077 0.00271345 0.02571925 0.00269657 0.00062294 0.11067271
     0.00524144 0.19651664 0.15383554 0.22888341 0.13083209 0.11240919
     0.01945599]
    [0.1851489  0.02698042 0.11626441 0.02168347 0.00496865 0.31242358
     0.02762129 0.07809107 0.00762381 0.07623999 0.04850533 0.06264847
     0.03180063]
    [0.008 0.014 0.012 0.002 0.    0.666 0.016 0.06  0.038 0.03  0.06  0.06
     0.034]
    [5.58484420e-03 2.58457182e-03 6.06856077e-03 1.18974807e-03
     1.64445226e-04 2.16018516e-01 2.26869048e-03 2.54855733e-01
     1.05620892e-01 2.65655795e-01 7.26437428e-02 5.64419007e-02
     1.09025601e-02]
  
```python
cols = X_train_scaled.columns.values
display(cols)
# Create a dataframe with features
feature_dataframe = pd.DataFrame( {'features': cols,
     'Random Forest feature importances': rf_feature,
     'Extra Trees  feature importances': et_feature,
      'AdaBoost feature importances': ada_feature,
    'Gradient Boost feature importances': gb_feature
    })

feature_dataframe
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AdaBoost feature importances</th>
      <th>Extra Trees  feature importances</th>
      <th>Gradient Boost feature importances</th>
      <th>Random Forest feature importances</th>
      <th>features</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.008</td>
      <td>0.185149</td>
      <td>0.005585</td>
      <td>0.010401</td>
      <td>Screen name length</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.014</td>
      <td>0.026980</td>
      <td>0.002585</td>
      <td>0.002713</td>
      <td>Number of digits in screen name</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.012</td>
      <td>0.116264</td>
      <td>0.006069</td>
      <td>0.025719</td>
      <td>User name length</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.002</td>
      <td>0.021683</td>
      <td>0.001190</td>
      <td>0.002697</td>
      <td>Default profile (binary)</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000</td>
      <td>0.004969</td>
      <td>0.000164</td>
      <td>0.000623</td>
      <td>Default picture (binary)</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.666</td>
      <td>0.312424</td>
      <td>0.216019</td>
      <td>0.110673</td>
      <td>Account age (days)</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.016</td>
      <td>0.027621</td>
      <td>0.002269</td>
      <td>0.005241</td>
      <td>Number of unique profile descriptions</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.060</td>
      <td>0.078091</td>
      <td>0.254856</td>
      <td>0.196517</td>
      <td>Number of friends</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.038</td>
      <td>0.007624</td>
      <td>0.105621</td>
      <td>0.153836</td>
      <td>Number of followers</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.030</td>
      <td>0.076240</td>
      <td>0.265656</td>
      <td>0.228883</td>
      <td>Number of favorites</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.060</td>
      <td>0.048505</td>
      <td>0.072644</td>
      <td>0.130832</td>
      <td>Number of tweets per hour</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.060</td>
      <td>0.062648</td>
      <td>0.056442</td>
      <td>0.112409</td>
      <td>Number of tweets total</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.034</td>
      <td>0.031801</td>
      <td>0.010903</td>
      <td>0.019456</td>
      <td>timing_tweet</td>
    </tr>
  </tbody>
</table>
</div>


<script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type="text/javascript">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: "STIX-Web"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>


```python
# Scatter plot 
trace = go.Scatter(
    y = feature_dataframe['Random Forest feature importances'].values,
    x = feature_dataframe['features'].values,
    mode='markers',
    marker=dict(
        sizemode = 'diameter',
        sizeref = 1,
        size = 25,
#       size= feature_dataframe['AdaBoost feature importances'].values,
        #color = np.random.randn(500), #set color equal to a variable
        color = feature_dataframe['Random Forest feature importances'].values,
        colorscale='Portland',
        showscale=True
    ),
    text = feature_dataframe['features'].values
)
data = [trace]


layout= go.Layout(
    autosize= True,
    title= 'Random Forest Feature Importance',
    hovermode= 'closest',
#     xaxis= dict(
#         title= 'Pop',
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig,filename='scatter2010')

# Scatter plot 
trace = go.Scatter(
    y = feature_dataframe['Extra Trees  feature importances'].values,
    x = feature_dataframe['features'].values,
    mode='markers',
    marker=dict(
        sizemode = 'diameter',
        sizeref = 1,
        size = 25,
#       size= feature_dataframe['AdaBoost feature importances'].values,
        #color = np.random.randn(500), #set color equal to a variable
        color = feature_dataframe['Extra Trees  feature importances'].values,
        colorscale='Portland',
        showscale=True
    ),
    text = feature_dataframe['features'].values
)
data = [trace]

layout= go.Layout(
    autosize= True,
    title= 'Extra Trees Feature Importance',
    hovermode= 'closest',
#     xaxis= dict(
#         title= 'Pop',
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig,filename='scatter2010')

# Scatter plot 
trace = go.Scatter(
    y = feature_dataframe['AdaBoost feature importances'].values,
    x = feature_dataframe['features'].values,
    mode='markers',
    marker=dict(
        sizemode = 'diameter',
        sizeref = 1,
        size = 25,
#       size= feature_dataframe['AdaBoost feature importances'].values,
        #color = np.random.randn(500), #set color equal to a variable
        color = feature_dataframe['AdaBoost feature importances'].values,
        colorscale='Portland',
        showscale=True
    ),
    text = feature_dataframe['features'].values
)
data = [trace]

layout= go.Layout(
    autosize= True,
    title= 'AdaBoost Feature Importance',
    hovermode= 'closest',
#     xaxis= dict(
#         title= 'Pop',
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig,filename='scatter2010')

# Scatter plot 
trace = go.Scatter(
    y = feature_dataframe['Gradient Boost feature importances'].values,
    x = feature_dataframe['features'].values,
    mode='markers',
    marker=dict(
        sizemode = 'diameter',
        sizeref = 1,
        size = 25,
#       size= feature_dataframe['AdaBoost feature importances'].values,
        #color = np.random.randn(500), #set color equal to a variable
        color = feature_dataframe['Gradient Boost feature importances'].values,
        colorscale='Portland',
        showscale=True
    ),
    text = feature_dataframe['features'].values
)
data = [trace]

layout= go.Layout(
    autosize= True,
    title= 'Gradient Boosting Feature Importance',
    hovermode= 'closest',
#     xaxis= dict(
#         title= 'Pop',
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig,filename='scatter2010')
```



<div id="0951e065-9b3e-4771-96d9-abec8f15c824" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("0951e065-9b3e-4771-96d9-abec8f15c824", [{"marker": {"color": [0.01040077, 0.00271345, 0.02571925, 0.00269657, 0.00062294, 0.11067271, 0.00524144, 0.19651664, 0.15383554, 0.22888341, 0.13083209, 0.11240919, 0.01945599], "colorscale": "Portland", "showscale": true, "size": 25, "sizemode": "diameter", "sizeref": 1}, "mode": "markers", "text": ["Screen name length", "Number of digits in screen name", "User name length", "Default profile (binary)", "Default picture (binary)", "Account age (days)", "Number of unique profile descriptions", "Number of friends", "Number of followers", "Number of favorites", "Number of tweets per hour", "Number of tweets total", "timing_tweet"], "x": ["Screen name length", "Number of digits in screen name", "User name length", "Default profile (binary)", "Default picture (binary)", "Account age (days)", "Number of unique profile descriptions", "Number of friends", "Number of followers", "Number of favorites", "Number of tweets per hour", "Number of tweets total", "timing_tweet"], "y": [0.01040077, 0.00271345, 0.02571925, 0.00269657, 0.00062294, 0.11067271, 0.00524144, 0.19651664, 0.15383554, 0.22888341, 0.13083209, 0.11240919, 0.01945599], "type": "scatter", "uid": "cb2daf4f-ee53-467a-9265-27ecf778bc7e"}], {"autosize": true, "hovermode": "closest", "showlegend": false, "title": "Random Forest Feature Importance", "yaxis": {"gridwidth": 2, "ticklen": 5, "title": "Feature Importance"}}, {"showLink": true, "linkText": "Export to plot.ly", "plotlyServerURL": "https://plot.ly"})});</script><script type="text/javascript">window.addEventListener("resize", function(){window._Plotly.Plots.resize(document.getElementById("0951e065-9b3e-4771-96d9-abec8f15c824"));});</script>



<div id="5ea11e12-13bc-4e34-af5e-86866866299c" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("5ea11e12-13bc-4e34-af5e-86866866299c", [{"marker": {"color": [0.1851489, 0.02698042, 0.11626441, 0.02168347, 0.00496865, 0.31242358, 0.02762129, 0.07809107, 0.00762381, 0.07623999, 0.04850533, 0.06264847, 0.03180063], "colorscale": "Portland", "showscale": true, "size": 25, "sizemode": "diameter", "sizeref": 1}, "mode": "markers", "text": ["Screen name length", "Number of digits in screen name", "User name length", "Default profile (binary)", "Default picture (binary)", "Account age (days)", "Number of unique profile descriptions", "Number of friends", "Number of followers", "Number of favorites", "Number of tweets per hour", "Number of tweets total", "timing_tweet"], "x": ["Screen name length", "Number of digits in screen name", "User name length", "Default profile (binary)", "Default picture (binary)", "Account age (days)", "Number of unique profile descriptions", "Number of friends", "Number of followers", "Number of favorites", "Number of tweets per hour", "Number of tweets total", "timing_tweet"], "y": [0.1851489, 0.02698042, 0.11626441, 0.02168347, 0.00496865, 0.31242358, 0.02762129, 0.07809107, 0.00762381, 0.07623999, 0.04850533, 0.06264847, 0.03180063], "type": "scatter", "uid": "afeb5361-d886-4ab5-a81e-0945ace32597"}], {"autosize": true, "hovermode": "closest", "showlegend": false, "title": "Extra Trees Feature Importance", "yaxis": {"gridwidth": 2, "ticklen": 5, "title": "Feature Importance"}}, {"showLink": true, "linkText": "Export to plot.ly", "plotlyServerURL": "https://plot.ly"})});</script><script type="text/javascript">window.addEventListener("resize", function(){window._Plotly.Plots.resize(document.getElementById("5ea11e12-13bc-4e34-af5e-86866866299c"));});</script>



<div id="8f077645-f8d2-41d0-9e63-2e9aa9d4c72a" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("8f077645-f8d2-41d0-9e63-2e9aa9d4c72a", [{"marker": {"color": [0.008, 0.014, 0.012, 0.002, 0.0, 0.666, 0.016, 0.06, 0.038, 0.03, 0.06, 0.06, 0.034], "colorscale": "Portland", "showscale": true, "size": 25, "sizemode": "diameter", "sizeref": 1}, "mode": "markers", "text": ["Screen name length", "Number of digits in screen name", "User name length", "Default profile (binary)", "Default picture (binary)", "Account age (days)", "Number of unique profile descriptions", "Number of friends", "Number of followers", "Number of favorites", "Number of tweets per hour", "Number of tweets total", "timing_tweet"], "x": ["Screen name length", "Number of digits in screen name", "User name length", "Default profile (binary)", "Default picture (binary)", "Account age (days)", "Number of unique profile descriptions", "Number of friends", "Number of followers", "Number of favorites", "Number of tweets per hour", "Number of tweets total", "timing_tweet"], "y": [0.008, 0.014, 0.012, 0.002, 0.0, 0.666, 0.016, 0.06, 0.038, 0.03, 0.06, 0.06, 0.034], "type": "scatter", "uid": "8ed23aff-a343-43a3-b013-b2ba010cd79e"}], {"autosize": true, "hovermode": "closest", "showlegend": false, "title": "AdaBoost Feature Importance", "yaxis": {"gridwidth": 2, "ticklen": 5, "title": "Feature Importance"}}, {"showLink": true, "linkText": "Export to plot.ly", "plotlyServerURL": "https://plot.ly"})});</script><script type="text/javascript">window.addEventListener("resize", function(){window._Plotly.Plots.resize(document.getElementById("8f077645-f8d2-41d0-9e63-2e9aa9d4c72a"));});</script>



<div id="ec12e87c-3f10-475d-9aec-40a72938f3cc" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("ec12e87c-3f10-475d-9aec-40a72938f3cc", [{"marker": {"color": [0.0055848442, 0.00258457182, 0.00606856077, 0.00118974807, 0.000164445226, 0.216018516, 0.00226869048, 0.254855733, 0.105620892, 0.265655795, 0.0726437428, 0.0564419007, 0.0109025601], "colorscale": "Portland", "showscale": true, "size": 25, "sizemode": "diameter", "sizeref": 1}, "mode": "markers", "text": ["Screen name length", "Number of digits in screen name", "User name length", "Default profile (binary)", "Default picture (binary)", "Account age (days)", "Number of unique profile descriptions", "Number of friends", "Number of followers", "Number of favorites", "Number of tweets per hour", "Number of tweets total", "timing_tweet"], "x": ["Screen name length", "Number of digits in screen name", "User name length", "Default profile (binary)", "Default picture (binary)", "Account age (days)", "Number of unique profile descriptions", "Number of friends", "Number of followers", "Number of favorites", "Number of tweets per hour", "Number of tweets total", "timing_tweet"], "y": [0.0055848442, 0.00258457182, 0.00606856077, 0.00118974807, 0.000164445226, 0.216018516, 0.00226869048, 0.254855733, 0.105620892, 0.265655795, 0.0726437428, 0.0564419007, 0.0109025601], "type": "scatter", "uid": "497d8821-921c-4e7c-b4ac-abc2d252a572"}], {"autosize": true, "hovermode": "closest", "showlegend": false, "title": "Gradient Boosting Feature Importance", "yaxis": {"gridwidth": 2, "ticklen": 5, "title": "Feature Importance"}}, {"showLink": true, "linkText": "Export to plot.ly", "plotlyServerURL": "https://plot.ly"})});</script><script type="text/javascript">window.addEventListener("resize", function(){window._Plotly.Plots.resize(document.getElementById("ec12e87c-3f10-475d-9aec-40a72938f3cc"));});</script>




```python
# Create the new column containing the average of values
feature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise
feature_dataframe.head(3)
```





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AdaBoost feature importances</th>
      <th>Extra Trees  feature importances</th>
      <th>Gradient Boost feature importances</th>
      <th>Random Forest feature importances</th>
      <th>features</th>
      <th>mean</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.008</td>
      <td>0.185149</td>
      <td>0.005585</td>
      <td>0.010401</td>
      <td>Screen name length</td>
      <td>0.052284</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.014</td>
      <td>0.026980</td>
      <td>0.002585</td>
      <td>0.002713</td>
      <td>Number of digits in screen name</td>
      <td>0.011570</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.012</td>
      <td>0.116264</td>
      <td>0.006069</td>
      <td>0.025719</td>
      <td>User name length</td>
      <td>0.040013</td>
    </tr>
  </tbody>
</table>
</div>



### Plotly Barplot of Average Feature Importances

Having obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:



```python
y = feature_dataframe['mean'].values
x = feature_dataframe['features'].values
data = [go.Bar(
            x= x,
             y= y,
            width = 0.5,
            marker=dict(
               color = feature_dataframe['mean'].values,
            colorscale='Portland',
            showscale=True,
            reversescale = False
            ),
            opacity=0.6
        )]

layout= go.Layout(
    autosize= True,
    title= 'Barplots of Mean Feature Importance',
    hovermode= 'closest',
#     xaxis= dict(
#         title= 'Pop',
#         ticklen= 5,
#         zeroline= False,
#         gridwidth= 2,
#     ),
    yaxis=dict(
        title= 'Feature Importance',
        ticklen= 5,
        gridwidth= 2
    ),
    showlegend= False
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig, filename='bar-direct-labels')
```



<div id="acd04579-081d-4775-81d5-720a6de96a4e" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("acd04579-081d-4775-81d5-720a6de96a4e", [{"marker": {"color": [0.05228362855, 0.011569610455, 0.0400130551925, 0.006892447017499999, 0.0014390088065, 0.3262787015, 0.01278285512, 0.14736586075, 0.0762700605, 0.15019479875, 0.0779952907, 0.072874890175, 0.024039795024999998], "colorscale": "Portland", "reversescale": false, "showscale": true}, "opacity": 0.6, "width": 0.5, "x": ["Screen name length", "Number of digits in screen name", "User name length", "Default profile (binary)", "Default picture (binary)", "Account age (days)", "Number of unique profile descriptions", "Number of friends", "Number of followers", "Number of favorites", "Number of tweets per hour", "Number of tweets total", "timing_tweet"], "y": [0.05228362855, 0.011569610455, 0.0400130551925, 0.006892447017499999, 0.0014390088065, 0.3262787015, 0.01278285512, 0.14736586075, 0.0762700605, 0.15019479875, 0.0779952907, 0.072874890175, 0.024039795024999998], "type": "bar", "uid": "d7da44d0-cff4-48d8-9b8e-eaee246c8d1b"}], {"autosize": true, "hovermode": "closest", "showlegend": false, "title": "Barplots of Mean Feature Importance", "yaxis": {"gridwidth": 2, "ticklen": 5, "title": "Feature Importance"}}, {"showLink": true, "linkText": "Export to plot.ly", "plotlyServerURL": "https://plot.ly"})});</script><script type="text/javascript">window.addEventListener("resize", function(){window._Plotly.Plots.resize(document.getElementById("acd04579-081d-4775-81d5-720a6de96a4e"));});</script>


## Second-Level Predictions from the First-level Output

### First-level output as new features

Having now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.



```python
base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),
     'ExtraTrees': et_oof_train.ravel(),
     'AdaBoost': ada_oof_train.ravel(),
      'GradientBoost': gb_oof_train.ravel(),
        'Logistic Regression': logreg_oof_train.ravel()
    })
display(base_predictions_train.head())

base_predictions_test = pd.DataFrame( {'RandomForest': rf_oof_test.ravel(),
     'ExtraTrees': et_oof_test.ravel(),
     'AdaBoost': ada_oof_test.ravel(),
      'GradientBoost': gb_oof_test.ravel(),
        'Logistic Regression': logreg_oof_test.ravel()
    })
base_predictions_test.head()

```



<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AdaBoost</th>
      <th>ExtraTrees</th>
      <th>GradientBoost</th>
      <th>Logistic Regression</th>
      <th>RandomForest</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>





<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>AdaBoost</th>
      <th>ExtraTrees</th>
      <th>GradientBoost</th>
      <th>Logistic Regression</th>
      <th>RandomForest</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
  </tbody>
</table>
</div>



### Correlation Heatmap of the Second Level Training set



```python
data = [
    go.Heatmap(
        z= base_predictions_train.astype(float).corr().values ,
        x=base_predictions_train.columns.values,
        y= base_predictions_train.columns.values,
          colorscale='Viridis',
            showscale=True,
            reversescale = True
    )
]
py.iplot(data, filename='labelled-heatmap')
```



<div id="f671a1b5-41da-4dcc-8e27-fa0b7787f7af" style="height: 525px; width: 100%;" class="plotly-graph-div"></div><script type="text/javascript">require(["plotly"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL="https://plot.ly";Plotly.newPlot("f671a1b5-41da-4dcc-8e27-fa0b7787f7af", [{"colorscale": "Viridis", "reversescale": true, "showscale": true, "x": ["AdaBoost", "ExtraTrees", "GradientBoost", "Logistic Regression", "RandomForest"], "y": ["AdaBoost", "ExtraTrees", "GradientBoost", "Logistic Regression", "RandomForest"], "z": [[1.0, 0.4559080134594197, 0.9150387522621493, 0.5847178090204851, 0.8321541628801644], [0.4559080134594197, 1.0, 0.45190916367069717, 0.6175688026868718, 0.4766534320548621], [0.9150387522621493, 0.45190916367069717, 1.0, 0.5696372365346719, 0.8449251609186822], [0.5847178090204851, 0.6175688026868718, 0.5696372365346719, 1.0, 0.6275233207195023], [0.8321541628801644, 0.4766534320548621, 0.8449251609186822, 0.6275233207195023, 1.0]], "type": "heatmap", "uid": "dee2e024-84d0-4efb-9a55-8969be03c32b"}], {}, {"showLink": true, "linkText": "Export to plot.ly", "plotlyServerURL": "https://plot.ly"})});</script><script type="text/javascript">window.addEventListener("resize", function(){window._Plotly.Plots.resize(document.getElementById("f671a1b5-41da-4dcc-8e27-fa0b7787f7af"));});</script>




```python
#x_train = np.concatenate(( rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train, logreg_oof_train), axis=1)
#x_test = np.concatenate(( rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test, logreg_oof_test), axis=1)
#x_train = base_predictions_train.mean(axis=1)
#x_test = base_predictions_test.mean(axis=1)
x_train = np.concatenate((0.4*ada_oof_train, 0.2*svc_oof_train, 0.2*rf_oof_train, 0.35*logreg_oof_train), axis=1)
x_test = np.concatenate((0.4*ada_oof_test, 0.2*svc_oof_test, 0.2*rf_oof_test, 0.35*logreg_oof_test), axis=1)
x_train = x_train.mean(axis=1).reshape(-1,1)
x_test = x_test.mean(axis=1).reshape(-1,1)
```


There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores. Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model.

### Second level learning model via XGBoost

Here we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the official documentation.

Anyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:



```python
import xgboost as xgb
gbm = xgb.XGBClassifier(
    learning_rate = 0.001,
 n_estimators= 1000,
 max_depth= 5,
 min_child_weight= 1,
 gamma=0.8,                      
 subsample=0.7,
 colsample_bytree=0.9,
 objective= 'binary:hinge',
 nthread= -1,
 scale_pos_weight=1).fit(x_train, Y_train)
predictions = gbm.predict(x_test)
```


Just a quick run down of the XGBoost parameters used in the model:

**max_depth :** How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.

**gamma :** minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.

**eta :** step size shrinkage used in each boosting step to prevent overfitting



```python
gbm_train = gbm.score(x_train, Y_train)
gbm_test = gbm.score(x_test, Y_test)
print(gbm_test)
```


    0.9286866731203615
    

## Blending

In this section we utilize the `mlens` package to develop a blended ensemble with 3 layers and more than 20 different classification techniques in order to achieve a truly superior predictive capability to the previous models. The blended model is also surprisingly fast given that it contains so many different methods, including many not previously discussed in AC209a such as gaussian process classifiers, MLP classifiers, extremely randomized tree classifiers, naive Bayes classifiers, and more.

The purpose of showing the blended model is to see what happens when you (quite literally) throw everything you have at the problem and see how well you do. In this case, we do surprisingly well, but trying to optimize the hyperparameters of this model would be a truly daunting task, and there is essentially no interpretability in this model. It is purely to obtain maximal accuracy for the given input data.



```python
import xgboost as xgb
from sklearn.ensemble import VotingClassifier
from sklearn.cluster import KMeans
from mlens.ensemble import BlendEnsemble
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.neural_network import MLPClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestClassifier

ensemble = BlendEnsemble()
ensemble.add([SVR(C=.1), RandomForestClassifier(n_estimators=200, random_state=SEED), LogisticRegression(C=1),  ExtraTreesClassifier(max_depth=10,random_state=SEED), GradientBoostingClassifier(random_state=SEED), AdaBoostClassifier(random_state=SEED)])
#ensemble.add([SVC(C=1),GaussianProcessClassifier(random_state=SEED),LogisticRegression(C=1000000), GradientBoostingClassifier(random_state=SEED), AdaBoostClassifier(random_state=SEED)])
#ensemble.add([ RandomForestClassifier(n_estimators=100, random_state=SEED), GradientBoostingClassifier(random_state=SEED),GaussianProcessClassifier(random_state=SEED)])
#ensemble.add([SVC(C=1), AdaBoostClassifier(random_state=SEED)])
ensemble.add([xgb.XGBClassifier(random_state=SEED), RandomForestClassifier(n_estimators=200, random_state=SEED), GradientBoostingClassifier(random_state=SEED)])
ensemble.add_meta(xgb.XGBClassifier(random_state=SEED))

ensemble.fit(X_train_scaled, Y_train)
preds = ensemble.predict(X_test_scaled)
ensemble_test = accuracy_score(Y_test, preds)
ensemble_train = accuracy_score(Y_train, ensemble.predict(X_train_scaled))
print(ensemble_test)
```


    0.9474023878670539
    

## Summary of Models

The following dataframe shows each of the tested models and its corresponding accuracy on the testing set.



```python
dict_var = {#"training time": [dec_1_fit_time,random_1_fit_time,model_1_fit_time,model_xg1_fit_time,clf_1_fit_time,cat_1_fit_time],
            #"inference time": [dec_1_predict_time,random_1_predict_time,model_1_predict_time,model_xg1_predict_time,clf_1_predict_time,cat_1_predict_time],
            "test accuracy": [logreg_test, linearLogCVpoly_test, rf_test, knn_best_k_test ,lda_test,qda_test, adaboost_test, xgb_test, gbm_test, ensemble_test],
            "training accuracy": [logreg_train, linearLogCVpoly_train, rf_train, knn_best_k_train ,lda_train,qda_train, adaboost_train, xgb_train, gbm_train, ensemble_train],
            #"Cross validation": ['No','No','No','No','No','No']
           }
print("Performance comparison of the six methods:")
df_var = pd.DataFrame.from_dict(dict_var)
df_var.index= ['Linear LR', 'Polynomial LR', 'Random Forest', 'kNN', 'LDA', 'QDA', 'AdaBoost', 'XGBoost', 'Stacking (2nd-Level Model)', 'Blending (3rd-Level Model)']
display(df_var)
```


    Performance comparison of the six methods:
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>test accuracy</th>
      <th>training accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Linear LR</th>
      <td>0.775411</td>
      <td>0.776148</td>
    </tr>
    <tr>
      <th>Polynomial LR</th>
      <td>0.800581</td>
      <td>0.808937</td>
    </tr>
    <tr>
      <th>Random Forest</th>
      <td>0.913520</td>
      <td>0.993774</td>
    </tr>
    <tr>
      <th>kNN</th>
      <td>0.718296</td>
      <td>0.772690</td>
    </tr>
    <tr>
      <th>LDA</th>
      <td>0.707648</td>
      <td>0.705728</td>
    </tr>
    <tr>
      <th>QDA</th>
      <td>0.711197</td>
      <td>0.708080</td>
    </tr>
    <tr>
      <th>AdaBoost</th>
      <td>0.945789</td>
      <td>0.987548</td>
    </tr>
    <tr>
      <th>XGBoost</th>
      <td>0.922233</td>
      <td>0.936359</td>
    </tr>
    <tr>
      <th>Stacking (2nd-Level Model)</th>
      <td>0.928687</td>
      <td>0.935667</td>
    </tr>
    <tr>
      <th>Blending (3rd-Level Model)</th>
      <td>0.947402</td>
      <td>0.954759</td>
    </tr>
  </tbody>
</table>
</div>




```python
plt.figure(figsize=(14,10))
xx = range(len(df_var))
index_name=df_var.index
plt.bar(xx, df_var['test accuracy'], color='black', alpha=0.5)
plt.ylim(0.7,1)
plt.title('Model Comparison on Test Set', fontsize=18)
plt.ylabel('Test Accuracy (%)', fontsize=16)
plt.xticks(xx,index_name,rotation=90,fontsize = 16);
sns.despine()

plt.axhline(0.95, c='k', linewidth=3, linestyle='--');
```



![png](Advanced_Features_files/Final_Models_87_0.png){: .center}


The weighted stacked model performed the best on the test set, achieving an accuracy of 85.7%, a value more than 1% higher than that achieved by the other best models: logistic regression and random forest. With extra tuning of hyperparameters and model weightings it is likely that this could be increased further.

## Neural network


```python
model_NN = models.Sequential()

model_NN.add(layers.Dense(1000, input_shape=(X_train_scaled.shape[1],),
                activation='relu'))

model_NN.add(layers.Dense(350, input_shape=(X_train_scaled.shape[1],),
                activation='relu', 
                kernel_regularizer=regularizers.l2(0.01)))
model_NN.add(Dropout(0.5))
           
model_NN.add(layers.Dense(350,   
                activation='relu', 
                kernel_regularizer=regularizers.l2(0.01)))
model_NN.add(Dropout(0.5))


model_NN.add(layers.Dense(1,  
                activation='sigmoid')) 

model_NN.summary()
```


    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense (Dense)                (None, 1000)              14000     
    _________________________________________________________________
    dense_1 (Dense)              (None, 350)               350350    
    _________________________________________________________________
    dropout (Dropout)            (None, 350)               0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 350)               122850    
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 350)               0         
    _________________________________________________________________
    dense_3 (Dense)              (None, 1)                 351       
    =================================================================
    Total params: 487,551
    Trainable params: 487,551
    Non-trainable params: 0
    _________________________________________________________________
    



```python
tensorflow.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model_NN.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])
```




```python
%%capture
ANN_history = model_NN.fit(X_train_scaled, Y_train,batch_size=128,epochs=3000,validation_split=0.2)
```




```python
training_loss = ANN_history.history['loss']
epoch_count = range(1, len(training_loss) + 1)
plt.plot(epoch_count, training_loss, 'r--')
plt.legend(['Training Loss'])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show();
```



![png](Advanced_Features_files/Final_Models_94_0.png){: .center}




```python
fig, axs = plt.subplots(figsize=(8, 6))
plt.plot(ANN_history.history['acc'], '-o', label='train')
plt.plot(ANN_history.history['val_acc'], '-*', label='val')

plt.ylabel("Accuracy score")
plt.xlabel("Epoch")
plt.title("Accuracy score vs. Epochs")

plt.legend()
plt.show() 
```



![png](Advanced_Features_files/Final_Models_95_0.png){: .center}




```python
test_loss,test_acc = model_NN.evaluate(X_test_scaled, Y_test, verbose=1)
print('Test loss:', test_loss)
print('Test ACC:', test_acc)
```


    3099/3099 [==============================] - 0s 88us/step
    Test loss: 0.44289736228052284
    Test ACC: 0.8725395289379845

